{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder Class\n",
    "    Values:\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white (1 channel), so that's our default.\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, im_chan=1, output_chan=32, hidden_dim=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = output_chan\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, output_chan * 2, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a encoder block of the VAE, \n",
    "        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation\n",
    "        Parameters:\n",
    "        input_channels: how many channels the input feature representation has\n",
    "        output_channels: how many channels the output feature representation should have\n",
    "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "        stride: the stride of the convolution\n",
    "        final_layer: whether we're on the final layer (affects activation and batchnorm)\n",
    "        '''        \n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Encoder: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "        image: a flattened image tensor with dimension (im_dim)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        encoding = disc_pred.view(len(disc_pred), -1)\n",
    "        # The stddev output is treated as the log of the variance of the normal \n",
    "        # distribution by convention and for numerical stability\n",
    "        return encoding[:, :self.z_dim], encoding[:, self.z_dim:].exp()\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decoder Class\n",
    "    Values:\n",
    "    z_dim: the dimension of the noise vector, a scalar\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white, so that's our default\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(z_dim, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a Decoder block of the VAE, \n",
    "        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation\n",
    "        Parameters:\n",
    "        input_channels: how many channels the input feature representation has\n",
    "        output_channels: how many channels the output feature representation should have\n",
    "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "        stride: the stride of the convolution\n",
    "        final_layer: whether we're on the final layer (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Decoder: Given a noise vector, \n",
    "        returns a generated image.\n",
    "        Parameters:\n",
    "        noise: a noise tensor with dimensions (batch_size, z_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    VAE Class\n",
    "    Values:\n",
    "    z_dim: the dimension of the noise vector, a scalar\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white, so that's our default\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encode = Encoder(im_chan, z_dim)\n",
    "        self.decode = Decoder(z_dim, im_chan)\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Decoder: Given a noise vector, \n",
    "        returns a generated image.\n",
    "        Parameters:\n",
    "        images: an image tensor with dimensions (batch_size, im_chan, im_height, im_width)\n",
    "        Returns:\n",
    "        decoding: the autoencoded image\n",
    "        q_dist: the z-distribution of the encoding\n",
    "        '''\n",
    "        q_mean, q_stddev = self.encode(images)\n",
    "        q_dist = Normal(q_mean, q_stddev)\n",
    "        z_sample = q_dist.rsample() # Sample once from each distribution, using the `rsample` notation\n",
    "        decoding = self.decode(z_sample)\n",
    "        return decoding, q_dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence Lower Bound (ELBO)\n",
    "\n",
    "When training a VAE, you're trying to maximize the likelihood of the real images. What this means is that you'd like the learned probability distribution to think it's likely that a real image (and the features in that real image) occurs -- as opposed to, say, random noise or weird-looking things. And you want to maximize the likelihood of the real stuff occurring and appropriately associate it with a point in the latent space distribution prior $p(z)$ (more on this below), which is where your learned latent noise vectors will live. However, finding this likelihood explicitly is mathematically intractable. So, instead, you can get a good lower bound on the likelihood, meaning you can figure out what the worst-case scenario of the likelihood is (its lower bound which *is* mathematically tractable) and try to maximize that instead. Because if you maximize its lower bound, or worst-case, then you probably are making the likelihood better too. And this neat technique is known as maximizing the Evidence Lower Bound (ELBO).\n",
    "\n",
    "Some notation before jumping into explaining ELBO: First, the prior latent space distribution $p(z)$ is the prior probability you have on the latent space $z$. This represents the likelihood of a given latent point in the latent space, and you know what this actually is because you set it in the beginning as a multivariate normal distribution. Additionally, $q(z)$ refers to the posterior probability, or the distribution of the encoded images. Keep in mind that when each image is passed through the encoder, its encoding is a probability distribution.\n",
    "\n",
    "Knowing that notation, here's the mathematical notation for the ELBO of a VAE, which is the lower bound you want to maximize: $\\mathbb{E}\\left(\\log p(x|z)\\right) + \\mathbb{E}\\left(\\log \\frac{p(z)}{q(z)}\\right)$, which is equivalent to $\\mathbb{E}\\left(\\log p(x|z)\\right) - \\mathrm{D_{KL}}(q(z|x)\\Vert p(z))$\n",
    "\n",
    "ELBO can be broken down into two parts: the reconstruction loss $\\mathbb{E}\\left(\\log p(x|z)\\right)$ and the KL divergence term $\\mathrm{D_{KL}}(q(z|x)\\Vert p(z))$. You'll explore each of these two terms in the next code and text sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Loss \n",
    "\n",
    "Reconstruction loss refers to the distance between the real input image (that you put into the encoder) and the generated image (that comes out of the decoder). Explicitly, the reconstruction loss term is $\\mathbb{E}\\left(\\log p(x|z)\\right)$, the log probability of the true image given the latent value. \n",
    "\n",
    "For MNIST, you can treat each grayscale prediction as a binary random variable (also known as a Bernoulli distribution) with the value between 0 and 1 of a pixel corresponding to the output brightness, so you can use the binary cross entropy loss between the real input image and the generated image in order to represent the reconstruction loss term. \n",
    "\n",
    "In general, different assumptions about the \"distribution\" of the pixel brightnesses in an image will lead to different loss functions. For example, if you assume that the brightnesses of the pixels actually follow a normal distribution instead of a binary random (Bernoulli) distribution, this corresponds to a mean squared error (MSE) reconstruction loss.\n",
    "\n",
    "Why the mean squared error? Well, as a point moves away from the center, $\\mu$, of a normal distribution, its negative log likelihood increases quadratically. You can also write this as $\\mathrm{NLL}(x) \\propto (x-\\mu)^2$ for $x \\sim \\mathcal{N}(\\mu,\\sigma)$. As a result, assuming the pixel brightnesses are normally distributed implies an MSE reconstruction loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
